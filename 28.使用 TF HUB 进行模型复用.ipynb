{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c86442b8",
   "metadata": {},
   "source": [
    "小结\n",
    "    在这节课的学习之中，我们了解了什么是 TensorFlow Hub，同时我们了解了 TensorFlow Hub 内部所包含的模型的大致类别；然后我们学习了如何使用这些网络模型；最终我们通过一个简单的示例了解了其具体的用法。\n",
    "    \n",
    "    使用 TF HUB 进行模型复用\n",
    "    我们在之前的学习之中学习过如何进行模型的保存与重复的利用，之后我们又学习了如何进行迁移学习。那么今天我们就来认识一个专门用于 TensorFlow 模型复用的库 —— TensorFlow Hub。\n",
    "\n",
    "    1. 什么是 TensorFlow Hub 以及它的安装\n",
    "    TensorFlow Hub 是一个针对可重复使用的机器学习模块的库。 —— 官方定义\n",
    "    也就是说，TensorFlow Hub 内部含有一些已经预训练好的各种常用的网络模型与相应的权重，我们只需要调用其接口就可以得到相应的模型。\n",
    "    TensorFlow Hub 的出现可以说极大地降低了我们使用 TensorFlow 进行迁移学习的难度。因此，当想要使用 TensorFlow 进行迁移学习的时候，我们的第一选择应该是 TensorFlow Hub。\n",
    "    TensorFlow Hub 的安装也非常简单，我们只需要使用 pip 安装即可：\n",
    "       pip install tensorflow-hub \n",
    "    倘若下载速度缓慢，可以查阅之前的 TensorFlow 安装教程来使用国内镜像源进行下载。\n",
    "    \n",
    "    2. TensorFlow Hub 之中包含哪些模块\n",
    "TensorFlow HUB 之中包含的模块大致可以分为四个种类，他们分别是：文本模型、图像模型、音频模型、视频模型。\n",
    "    2.1. TensorFlow Hub 之中的文本模型\n",
    "    文本大类中又包含一些小的分类，比如分类、情感分析、语义分析等，这里介绍一些常用的模型：\n",
    "    - gnews-swivel-20dim/1 模型：文本任务的一个基本的迁移模型，包含 20 维的词向量；\n",
    "    - nnlm-en-dim50/1 模型：巨大的模型，包含 1M 个词汇量与 50 维的词向量；\n",
    "    - nnlm-en-dim128/1 模型：非常巨大的模型，包含 1M 个词汇量与 128 维词向量；\n",
    "    - BERT：迄今为止最大的文本处理模型。\n",
    "    使用以上几种模型，可以满足我们绝大多数的文本任务的迁移学习的需求。\n",
    "\n",
    "   2. 2. TensorFlow Hub 之中的图像模型\n",
    "    图像模型最主要是进行特征提取而使用的模型，我们一般会摒弃掉这些模型的最后几个网络层，从而达到特征提取的目的。\n",
    "    - mobilenet_v2：一个小型的迁移学习模型，可以胜任一些基本的任务；\n",
    "    - arbitrary-image-stylization：一个用来进行图像风格处理的模型；\n",
    "    - ResNet：规模从小到大都有，是最常用的特征提取模型。\n",
    "    \n",
    "    2.3. TensorFlow Hub 之中的音频模型\n",
    "    在音频的处理之中，我们最常用的网络模型就是 “spice” 模型，该模型能够很好地对音频信号进行特征的提取。\n",
    "    另外，我们可以使用 Librosa 等工具提取出 MFCC 等特征之后，再使用 ResNet 等图像特征提取模型进行下一步的处理。\n",
    "\n",
    "    2.4. TensorFlow Hub 之中的视频模型\n",
    "    在视频处理之中，我们最常用的方式是将视频分为独立的帧，然后对于每一帧使用图像模型进行特征的提取，从而将其转化为一个图片特征提取问题。\n",
    "    \n",
    "    3. 如何使用 TensorFlow Hub\n",
    "    我们要使用 TensorFlow Hub 之中的网络模型，我们只需要知道该模型对应的地址即可，而这些地址可以通过 TensorFlow Hub 官方网站进行查询。\n",
    "    在使用的过程之中，我们只需要经过如下几个步骤：\n",
    "    - 找到模型对应的 URL ；\n",
    "    - 使用 hub 对应的 API 来下载该网络模型并进行载入，比如我们常用的 Keras 模型可以通过 hub.KerasLayer API 来进行调用；\n",
    "    - 将加载的预训练模型加入到自己的网络之中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow-hub as hub\n",
    "\n",
    "'''\n",
    "这个用法打包起来了，好简单！\n",
    "'''\n",
    "model = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[20], input_shape=[],  dtype=tf.string, trainable=True)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ff67deb",
   "metadata": {},
   "source": [
    "    4. 使用 TensorFlow Hub 进行迁移学习的文本分类实例\n",
    "    在这一小结，我们会使用 TensorFlow Hub 的一个内置的文本网络模型来进行文本的分类任务的学习。\n",
    "    在这里，我们依然采用 IMDB 分类的任务来进行演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 定义基本参数\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=BATCH_SIZE, as_supervised=True)\n",
    "\n",
    "train_examples, train_labels = tfds.as_numpy(train_data)\n",
    "test_examples, test_labels = tfds.as_numpy(test_data)\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "model = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[20], input_shape=[],  dtype=tf.string, trainable=True)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练\n",
    "history = model.fit(train_examples, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 测试\n",
    "results = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6f8b457",
   "metadata": {},
   "source": [
    "最终我们的得到的结果为：\n",
    "Model: \"sequential_2\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "keras_layer_2 (KerasLayer)   (None, 20)                400020    \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 16)                336       \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 1)                 17        \n",
    "=================================================================\n",
    "Total params: 400,373\n",
    "Trainable params: 400,373\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Epoch 1/30\n",
    "391/391 [==============================] - 5s 12ms/step - loss: 1.5181 - accuracy: 0.6126\n",
    "Epoch 2/30\n",
    "391/391 [==============================] - 5s 13ms/step - loss: 0.7655 - accuracy: 0.6738\n",
    ".........\n",
    "Epoch 30/30\n",
    "391/391 [==============================] - 5s 12ms/step - loss: 0.0454 - accuracy: 0.9963\n",
    "782/782 [==============================] - 3s 4ms/step - loss: 1.5898 - accuracy: 0.8366\n",
    "\n",
    "[1.589829683303833, 0.8365600109100342]\n",
    "    可以看到，我们的模型一共包括三层，刚好符合我们的预期，同时我们的模型最终也在测试集合上达到了 83% 的准确率，而这是一个良好的结果。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
