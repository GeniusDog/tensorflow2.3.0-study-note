{
 "cells": [
  {
   "cell_type": "raw",
   "id": "48c137e5",
   "metadata": {},
   "source": [
    "小结\n",
    "    在这节课之中，我们学习了如何自定义网络层，如何指定网络层参数是否训练，如何进行运行模式的指定以及如何进行网络层嵌套等，最后我们又学习了如何进行自定义网络的构建。\n",
    "    \n",
    "   在上一节之中，我们学习了如何进行微分操作以及梯度的求导。那么这节课我们便开始自定义的下一步 —— 自定义网络层与模型。\n",
    "    这节课主要分为两个大部分进行讲解，它们分别是：\n",
    "        a.自定义网络层；\n",
    "        b.自定义模型。\n",
    "    我们会分别对这两个方面进行讲解。而网络层是模型的基础，因此我们会对网络层进行着重的讲解。\n",
    "    1. 如何自定义网络层\n",
    "    在 TensorFlow 之中，我们目前一般采用创建 keras 层的方式来进行网络层的构建，因此，我们的创建步骤大致分为以下几步：\n",
    "    a.定义网络层的类并继承 tf.keras.layers.Layer 类；\n",
    "    b.在初始化方法或者 build 方法之中定义网络的参数；\n",
    "    c.在 call 函数之中编写具体的处理流程。\n",
    "    在第二步之中，我们可以在初始化或者 build 方法之中定义网络的参数，两者的效果在目前的教程之中时一样的，因此为了简单起见，我们统一在初始化函数之中定义我们的网络参数。\n",
    "    以下是一个简单的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ae083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "自定义类：网络层类，继承Layer\n",
    "需要重写里面的两个方法：__init__、call\n",
    "\n",
    "定义了一个网络层，该网络层的数学形式为：\n",
    "y = w*(x**2) + b\n",
    "'''\n",
    "class MyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, input_units):\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_units, hidden_units), initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(hidden_units,), initializer=\"random_normal\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(tf.matmul(inputs, inputs), self.w) + self.b\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df8b4557",
   "metadata": {},
   "source": [
    "    通过代码我们可以发现，我们在初始化函数之中进行了参数的初始化操作，然后再在 call 方式之中进行了具体的计算。\n",
    "    对于添加参数，我们最常用的方式是采用 add_weight 方法来进行的；该方法最常用的参数有两个：\n",
    "    - shape: 表示该参数的形状，比如 3x3 等；\n",
    "    - initializer: 初始化器，一般为 “zeros”（初始化为 0），或者 “random_normal”（随机初始化）。\n",
    "    我们可以通过具体的数据进行查看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700121ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((5, 5))\n",
    "my_layer = MyLayer(10, 5)\n",
    "y = my_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56c5e1a2",
   "metadata": {},
   "source": [
    "输出为：\n",
    "tf.Tensor(\n",
    "[[ 0.5975663   0.46636996 -0.4837241   0.3024946   0.33699942  0.1420103\n",
    "   0.07284987 -0.5888218   0.13172552  0.01993698]\n",
    " [ 0.5975663   0.46636996 -0.4837241   0.3024946   0.33699942  0.1420103\n",
    "   0.07284987 -0.5888218   0.13172552  0.01993698]\n",
    " [ 0.5975663   0.46636996 -0.4837241   0.3024946   0.33699942  0.1420103\n",
    "   0.07284987 -0.5888218   0.13172552  0.01993698]\n",
    " [ 0.5975663   0.46636996 -0.4837241   0.3024946   0.33699942  0.1420103\n",
    "   0.07284987 -0.5888218   0.13172552  0.01993698]\n",
    " [ 0.5975663   0.46636996 -0.4837241   0.3024946   0.33699942  0.1420103\n",
    "   0.07284987 -0.5888218   0.13172552  0.01993698]], shape=(5, 10), dtype=float32)\n",
    "\n",
    "    2. 网络层中的固定参数\n",
    "    在上面的例子中，我们学习到了如何对自己的网络层添加参数，但是我们会发现，我们刚刚添加的参数都是可以进行训练的参数，那么如何添加不可训练的参数呢？\n",
    "    我们可以在添加参数的方法之中添加属性 trainable："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.b = self.add_weight(shape=(hidden_units,), initializer=\"random_normal\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们可以通过以下代码进行实践：\n",
    "class MyLayer2(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, input_units):\n",
    "        super(MyLayer2, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_units, hidden_units), initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(hidden_units,), initializer=\"random_normal\", trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(tf.matmul(inputs, inputs), self.w) + self.b\n",
    "\n",
    "my_layer2 = MyLayer2(10, 5)\n",
    "print(len(my_layer.trainable_weights))   #2\n",
    "print(len(my_layer2.trainable_weights))  #1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "960cd0d8",
   "metadata": {},
   "source": [
    "    在该程序之中，我们定义了两个模型，一个模型的 b 可训练，另一个模型的 b 不可训练，于是我们可以发现，两个模型的可训练参数不一样，说明我们的属性起到了相应的效果。\n",
    "    \n",
    "    3. 决定网络是否进行训练\n",
    "    我们的网络在使用的时候包括两个情景：\n",
    "    a.训练的情景，需要我们进行参数的调整等；\n",
    "    b.测试的情景或其他情景，固定参数，只进行输出。\n",
    "    c.在大多数时间，我们都需要在训练或者测试的过程之中做一些额外的操作。\n",
    "    而我们的网络是默认进行训练的，那么如何才能将我们的网络调整为测试状态或是训练状态呢？答案就是 call 方法的参数：training。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da740319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer2(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, input_units):\n",
    "        super(MyLayer2, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_units, hidden_units), initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(hidden_units,), initializer=\"random_normal\", trainable=False)\n",
    "\n",
    "    # training=True\n",
    "    def call(self, inputs, training=True):\n",
    "        if training:\n",
    "          self.b = self.b * 2\n",
    "          # ...... Other Operations\n",
    "        return tf.matmul(tf.matmul(inputs, inputs), self.w) + self.b\n",
    "\n",
    "#通过如下方式来具体使用：\n",
    "# 于是我们在第一调用的时候进行训练，而第二次调用的时候不进行训练。\n",
    "my_layer2 = MyLayer2(10, 5)\n",
    "y = my_layer2(x, traing=True)\n",
    "y = my_layer2(x, traing=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be560a2c",
   "metadata": {},
   "source": [
    "4. 网络层的嵌套使用\n",
    "    在网络层的使用之中，我们可能会遇到网络层嵌套使用的情况。而且 TensorFlow 也可以支持网络层的嵌套使用。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "前面重新定义了两个网络层类，并在后面我们嵌套了我们之前的两个网络层，\n",
    "我们通过顺序调用来实现了一个新的网络层的操作。\n",
    "'''\n",
    "class MyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, input_units):\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_units, hidden_units), initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(hidden_units,), initializer=\"random_normal\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(tf.matmul(inputs, inputs), self.w) + self.b\n",
    "\n",
    "class MyLayer2(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, input_units):\n",
    "        super(MyLayer2, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_units, hidden_units), initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(hidden_units,), initializer=\"random_normal\", trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "class MyLayer3(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyLayer3, self).__init__()\n",
    "        self.l1 = MyLayer(10, 5)\n",
    "        self.l2 = MyLayer2(5, 10)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.l1(inputs)\n",
    "        y = self.l2(x)\n",
    "        return y\n",
    "\n",
    "# 我们可以通过具体的数据进行测试：\n",
    "x = tf.ones((5, 5))\n",
    "my_layer = MyLayer3(10, 5)\n",
    "y = my_layer(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce578a1f",
   "metadata": {},
   "source": [
    "tf.Tensor(\n",
    "[[ 0.00422265  0.02767846  0.04585129  0.10204907 -0.08051172]\n",
    " [ 0.00422265  0.02767846  0.04585129  0.10204907 -0.08051172]\n",
    " [ 0.00422265  0.02767846  0.04585129  0.10204907 -0.08051172]\n",
    " [ 0.00422265  0.02767846  0.04585129  0.10204907 -0.08051172]\n",
    " [ 0.00422265  0.02767846  0.04585129  0.10204907 -0.08051172]], shape=(5, 5), dtype=float32)\n",
    "\n",
    "    5. 自定义模型\n",
    "    既然了解了如何自定义网络层，那么便要知道如何自定义网络模型，其实网络模型是由网络层构成的，因此只要将网络层定义清楚，那么网络模型便可以很轻松得到了。\n",
    "    网络模型的构建大致也分为以下几步：\n",
    "    a.自定义模型类，并继承自 tf.keras.Model 类；\n",
    "    b.在初始化函数之中定义要用到的网络层；\n",
    "    c.在 call 函数之中定义具体的处理方式。\n",
    "    于是，借用上面的网络层，我们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d7e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以定义我们的网络模型：\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.l1 = MyLayer(10, 5)\n",
    "        self.l2 = MyLayer2(5, 10)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.l1(inputs)\n",
    "        y = self.l2(x)\n",
    "        return y\n",
    "\n",
    "# 我们可以通过测试：\n",
    "x = tf.ones((5, 5))\n",
    "model = MyModel()\n",
    "y = model(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca1cd941",
   "metadata": {},
   "source": [
    "    我们的网络模型使用了 MyLayer 和 MyLayer2 两个网络层，并且在 call 函数之中进行了顺序处理，从而得到最后的结果。\n",
    "得到输出：\n",
    "    tf.Tensor(\n",
    "[[ 0.07020754 -0.14177878 -0.00841531 -0.0398875   0.14821295]\n",
    " [ 0.07020754 -0.14177878 -0.00841531 -0.0398875   0.14821295]\n",
    " [ 0.07020754 -0.14177878 -0.00841531 -0.0398875   0.14821295]\n",
    " [ 0.07020754 -0.14177878 -0.00841531 -0.0398875   0.14821295]\n",
    " [ 0.07020754 -0.14177878 -0.00841531 -0.0398875   0.14821295]], shape=(5, 5), dtype=float32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
