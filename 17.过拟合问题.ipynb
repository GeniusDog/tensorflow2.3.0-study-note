{
 "cells": [
  {
   "cell_type": "raw",
   "id": "95fb1d5b",
   "metadata": {},
   "source": [
    "小结:\n",
    "    在这节课之中，我们学习了什么是过拟合，同时了解了如何在 TensorFlow 之中避免过拟合的发生，我们可以采用的方法有 DropOut 、正则化以及早停策略。而在实际的应用之中，大家可以根据自己需要将不同的方法结合起来使用，这样才能达到比较良好的效果\n",
    "\n",
    "    在我们之前的学习之中，我们或多或少都会遇到一些训练时间的问题。比如“训练时间越长是不是最后的结果就会越好？”等问题。答案当然是否定的，这是因为在训练的过程之中会遭遇到“过拟合”的问题，这是一种随着训练时间不断加长而产生的问题，那么这节课我们就来学习一下什么是过拟合，同时了解一下 TensorFlow 之中的避免过拟合的简单的方法。\n",
    "    数据集：猫狗分类的例子进行示例演示。\n",
    "    \n",
    "    1. 什么是过拟合\n",
    "    过拟合，简单来说就是“学习过度”，也就是说模型在训练集合上的精度越来越高，但是却在测试集上的精度越来越低的情况。\n",
    "    这是因为网络模型在训练集合上学习到了太多的“没用的”特征，以至于模型的泛化能力下降。如下面两幅图所示，其中蓝色代表训练集合上的指标，而黄色代表测试集合上的指标。\n",
    "    一般折线图：蓝色代表训练集合上的指标，而黄色代表测试集合上的指标。\n",
    "    常见过拟合的情况：\n",
    "    在准确率的曲线中，随着不断地训练，模型在训练集合上的准确率逐渐逼近100%，而训练集合上的准确率却一直在70%徘徊。\n",
    "    在损失 Loss 的曲线中，随着不断地训练，模型在训练集合上的损失逐渐逼近0，而训练集合上的损失却在某次迭代之后不断升高。\n",
    "    那么接下来我们就来学习一下如何在 TensorFlow 之中简单地避免过拟合。这节课之中，我们要学习的方法有三种：\n",
    "    那么接下来我们就来学习一下如何在 TensorFlow 之中简单地避免过拟合。这节课之中，我们要学习的方法有三种：\n",
    "        使用 DropOut ；\n",
    "        使用正则化；\n",
    "        使用早停策略。\n",
    "        \n",
    "     2. 使用 DropOut\n",
    "    在产生过拟合的原因之中，一个重要的原因就是“网络参数过多”，也就是网络模型的学习能力过强，从而导致它会学习到很多没用的信息，从而导致过拟合情况的发生。而使用 DropOut 就是在一定程度上降低网络参数，降低它的学习能力。\n",
    "\n",
    "    它的实现比较简单:\n",
    "    tf.keras.layers.Dropout(frac)\n",
    "    可以看出，它是一个网络层，它的参数 frac 是一个 0 到 1 的小数，该网络层会按照 frac 的概率随机丢掉一些参数，从而达到降低网络参数数量的目的。在使用的过程之中，我们只需要将该网络层嵌入到模型的需要 DropOut 的网络层之前即可。\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入Dropout加在每一次单元模块层之后，最后一层不需要加\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu',\n",
    "                input_shape=(Height, Width ,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39a88d7c",
   "metadata": {},
   "source": [
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_3 (Conv2D)            (None, 128, 128, 16)      448       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_3 (MaxPooling2 (None, 64, 64, 16)        0         \n",
    "_________________________________________________________________\n",
    "dropout (Dropout)            (None, 64, 64, 16)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_4 (Conv2D)            (None, 64, 64, 32)        4640      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 32)        0         \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_5 (Conv2D)            (None, 32, 32, 64)        18496     \n",
    "_________________________________________________________________\n",
    "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 64)        0         \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 16, 16, 64)        0         \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 16384)             0         \n",
    "_________________________________________________________________\n",
    "dropout_3 (Dropout)          (None, 16384)             0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 512)               8389120   \n",
    "_________________________________________________________________\n",
    "dropout_4 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 1)                 513       \n",
    "=================================================================\n",
    "Total params: 8,413,217\n",
    "Trainable params: 8,413,217\n",
    "Non-trainable params: 0\n",
    "与之前的模型相比，我们现在的模型在一定程度上降低了过拟合。对于准确率，它并没有像之前一样保持徘徊，而是和训练集保持了一致；对于损失，它也没有上升，反而是一直处于一个较低的值。\n",
    "\n",
    "    3. 使用正则化\n",
    "    正则化是一种比较高级的防止过拟合产生的方法。它是通过网络的参数来计算网络的“代价”，然后将代价最小化来实现降低网络规模的目的。它主要包括两种方式， L1 正则化与 L2 正则化，这两种方式都涉及到很多的数学原理，因此这里不做过多的展开，我们可以进行一个简单的区分：\n",
    "        L1 正则化，代价与网络参数成正比；\n",
    "        L2 正则化，代价与网络参数的平方成正比。\n",
    "        而在实践的过程之中，我们最常使用的就是 L2 正则化。\n",
    "    具体来说，我们可以通过将支持正则化的网络层添加相应的正则化参数即可实现该网络层的正则化。比如对于 Dense 网络层来说，我们可以添加参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其中的 0.001 参数就是“代价”与网络参数的平方成正比的参数\n",
    "# 代价 = 0.001 * (网络参数**2)\n",
    "tf.keras.laysers.Dense(64, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "\n",
    "'''\n",
    "于是我们可以将我们的模型再次修改为：\n",
    "'''\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu',\n",
    "                input_shape=(Height, Width ,3),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f4bd42a",
   "metadata": {},
   "source": [
    "我们可以看到网络的模型结构为:\n",
    "Model: \"sequential_2\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_9 (Conv2D)            (None, 128, 128, 16)      448       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_9 (MaxPooling2 (None, 64, 64, 16)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_10 (Conv2D)           (None, 64, 64, 32)        4640      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_10 (MaxPooling (None, 32, 32, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_11 (Conv2D)           (None, 32, 32, 64)        18496     \n",
    "_________________________________________________________________\n",
    "max_pooling2d_11 (MaxPooling (None, 16, 16, 64)        0         \n",
    "_________________________________________________________________\n",
    "flatten_3 (Flatten)          (None, 16384)             0         \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 512)               8389120   \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 1)                 513       \n",
    "=================================================================\n",
    "Total params: 8,413,217\n",
    "Trainable params: 8,413,217\n",
    "Non-trainable params: 0\n",
    "\n",
    "    注意：我们可以发现，网络的参数并没有发生变化，这是因为正则化并不会引入新的参数，也不会减少参数。\n",
    "    \n",
    "    4. 使用早停策略\n",
    "    这个策略会使用到我们下节课学习到的回调函数，但是这也是方式过拟合产生的一种手段。它的思想比较简单：\n",
    "    “如果你在验证集上的准确率或者损失持续没有提升，那么我就把你停止掉，不让你继续训练。”\n",
    "    在 TensorFlow 之中，我们可以通过以下的回调方式来实现早停："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "其中EarlyStopping的常用参数包括：\n",
    "\n",
    "monitor: 指定哪一个指标作为监控的标准，一般为损失或者准确率，这里是损失；\n",
    "patience：忍耐限度，如果经过了 patience 个 epoch ，monitor 指标还没有提升，那么会停止训练。\n",
    "'''\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# 于是我们可以将模型还原为之前的模型，同时在训练的代码中添加相应的早停回调。\n",
    "# 在这里我们在训练的过程之中添加了一个EarlyStopping的回调。\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "history = model.fit_generator(\n",
    "    train_data_generator,\n",
    "    steps_per_epoch=TRAIN_NUM // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_data_generator,\n",
    "    validation_steps=VALID_NUM // BATCH_SIZE,\n",
    "    callbacks=[callback])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
