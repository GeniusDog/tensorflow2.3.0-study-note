{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0886199c",
   "metadata": {},
   "source": [
    "小结：\n",
    "    回调函数可以帮助我们更好地知道模型中参数的情况，更好地输出需要的信息。\n",
    "   回调函数是 TensorFlow 训练之中非常重要的一部分，我们在之前的学习之中或多或少地用到了回调函数。比如在之前的过拟合一节之中，我们就曾经用到了早停回调。那么这节课我们就来学习以下 TensorFlow 之中的回调函数。\n",
    "    简单来说，回调函数就是在训练到一定阶段的时候而执行的函数，我们最常采用的策略是每个Epoch结束之后执行一次回调函数。\n",
    "\n",
    "    回调函数的绝大多数 API 集中在 tf.keras.callbacks 之中，也就是说这是 Keras 之中的一个 API 。由于之前已经学习过早停回调，这节课我们来学习一下其他的几个常用的回调：\n",
    "    模型保存回调：tf.keras.callbacks.ModelCheckpoint；\n",
    "    学习率回调；tf.keras.callbacks.LearningRateScheduler；\n",
    "    自定义回调：tf.keras.callbacks.CallBack。\n",
    "    对于回调的使用方法，也是非常简单的，假设以下的数组之中定义了我们所需要的全部回调函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0098dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义使用哪一个回调函数，结果保存在数组\n",
    "callbacks = [......]\n",
    "\n",
    "# 我们在使用回调的时候，之中只需要在训练函数中指定回调即可：\n",
    "model.fit(..., ..., callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    2. 模型保存回调\n",
    "    模型保存的回调函数为\n",
    "    \n",
    "    path: 保存模型的路径；\n",
    "    monitor: 用哪个指标来评价模型的好坏，默认是验证集上的损失；\n",
    "    verbose: 输出日志的等级，只能为 0 或 1；\n",
    "    save_best_only: 是否只保存最好的模型，模型的好坏由 monitor 指定；\n",
    "    save_weights_only: 是否只保存权重，默认 False ，也就是保存整个模型；\n",
    "    save_freq: 保存的频率，可以为 ‘Epoch’ 或者一个整数，\n",
    "                默认为每个 Epoch 保存一次模型；\n",
    "                若是一个整数N，则是每训练 N 个 Batch 保存一次模型。\n",
    "'''\n",
    "tf.keras.callbacks.ModelCheckpoint(\n",
    "    path, monitor='val_loss', verbose=0, save_best_only=False,\n",
    "    save_weights_only=False, save_freq='epoch')\n",
    "\n",
    "'''\n",
    "3. 学习率回调\n",
    "\n",
    "其中 verbose 参数仍然是日志输出的等级，默认为 0 ；\n",
    "而 schedule 则是一个函数，用来定义一个学习率的变化。\n",
    "'''\n",
    "tf.keras.callbacks.LearningRateScheduler(\n",
    "    schedule, verbose=0\n",
    ")\n",
    "\n",
    "# 其中 schedule 函数的一个示例如下所示：\n",
    "# 学习率回调是在 20 个 Epoch 之前学习率保持不变\n",
    "# 而在 20 个 Epoch 之后，每个 Epoch 学习率变为原来的 0.1 。\n",
    "def my_schedule(epoch, lr):\n",
    "  if epoch < 20:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * 0.1\n",
    "\n",
    "'''\n",
    "4. 自定义回调\n",
    "    我们在使用回调的过程之中难免会遇到要自定义回调的情况，\n",
    "    这时我们便需要编写类来继承 tf.keras.callbacks.CallBack 类，从而实现我们的自定义回调。\n",
    "\n",
    "\n",
    "'''\n",
    "# 使用其中两个简单的函数来做一个简单的示例\n",
    "# 我们便可以在每次训练开始，以及每个 Epoch 开始之时进行输出日志\n",
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(\"Start epoch {}.\".format(epoch))\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"Starting training.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56ab1d97",
   "metadata": {},
   "source": [
    "在自定义回调的过程之中，你可以覆写不同的函数，从而可以实现在不同的时间来运行我们自定义的函数，这些函数包括：\n",
    "\n",
    "on_train_begin(self, logs=None): 在训练开始时调用；\n",
    "on_test_begin(self, logs=None): 在测试开始时调用；\n",
    "on_predict_begin(self, logs=None): 在预测开始时调用；\n",
    "on_train_end(self, logs=None) 在训练结束时调用；\n",
    "on_test_end(self, logs=None) 在测试结束时调用；\n",
    "on_predict_end(self, logs=None) 在预测结束时调用；\n",
    "on_train_batch_begin(self, batch, logs=None) 在训练期间的每个批次之前调用；\n",
    "on_test_batch_begin(self, batch, logs=None) 在测试期间的每个批次之前调用；\n",
    "on_predict_batch_begin(self, batch, logs=None) 在预测期间的每个批次之前调用；\n",
    "on_train_batch_end(self, batch, logs=None) 在训练期间的每个批次之后调用；\n",
    "on_test_batch_end(self, batch, logs=None) 在测试期间的每个批次之后调用；\n",
    "on_predict_batch_end(self, batch, logs=None) 在预测期间的每个批次之后调用；\n",
    "on_epoch_begin(self, epoch, logs=None) 在每次迭代训练开始时调用；\n",
    "on_epoch_end(self, epoch, logs=None) 在每次迭代训练结束时调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846235cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里，我们将同时使用模型保存回调、学习率回调以及自定义回调来做一个简单的示例：\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "# 这里在编译的时候指定了优化器的学习率是多少\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# 控制学习率\n",
    "def my_schedule(epoch, lr):\n",
    "  print('Learning rate: ' + str(lr))\n",
    "  if epoch < 5:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * 0.1\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(my_schedule)\n",
    "\n",
    "save_model_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/model/', save_weights_only=True, verbose=1,\n",
    "    monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(\"Start epoch {}.\".format(epoch))\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"Starting training.\")\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "    batch_size=64, epochs=10,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[MyCallback(), lr_callback, save_model_callback],\n",
    ")\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2aae95b0",
   "metadata": {},
   "source": [
    "    在这里，我们按照之前学习的方法定义了三个回调函数，分别是模型保存回调、学习率回调、以及自定义回调。其中模型保存回调会在每次训练后保存模型、学习率回调会在第五个 Epoch 之后便每个 Epoch 变为原来的 0.1 ，而自定义回调会在训练开始之前、每个 Epoch 开始之前输出相应的信息。\n",
    "    我们的三个回调函数都能正确地输出相应的信息，说明我们的回调函数已经成功生效。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3ea4ee7",
   "metadata": {},
   "source": [
    "在云平台运行的结果：\n",
    "模型网络结构：\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "flatten (Flatten)            (None, 784)               0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 64)                50240     \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                650       \n",
    "=================================================================\n",
    "Total params: 50,890\n",
    "Trainable params: 50,890\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "​\n",
    "回调函数中保存参数：模型、学习率、自定义回调\n",
    "Starting training.\n",
    "Start epoch 0.\n",
    "Learning rate: 0.009999999776482582\n",
    "Epoch 1/10\n",
    "936/938 [============================>.] - ETA: 0s - loss: 731.7809\n",
    "Epoch 00001: val_loss improved from inf to 18.24045, saving model to /model/\n",
    "938/938 [==============================] - 23s 24ms/step - loss: 730.6422 - val_loss: 18.2404\n",
    "Start epoch 1.\n",
    "Learning rate: 0.009999999776482582\n",
    "Epoch 2/10\n",
    "936/938 [============================>.] - ETA: 0s - loss: 13.9846\n",
    "Epoch 00002: val_loss improved from 18.24045 to 10.78909, saving model to /model/\n",
    "938/938 [==============================] - 21s 23ms/step - loss: 13.9760 - val_loss: 10.7891\n",
    "Start epoch 2.\n",
    "Learning rate: 0.009999999776482582\n",
    "Epoch 3/10\n",
    "938/938 [==============================] - ETA: 0s - loss: 9.3977- ETA: 1s - loss: 9.460 - ET\n",
    "Epoch 00003: val_loss improved from 10.78909 to 8.56772, saving model to /model/\n",
    "938/938 [==============================] - 30s 32ms/step - loss: 9.3977 - val_loss: 8.5677\n",
    "Start epoch 3.\n",
    "Learning rate: 0.009999999776482582\n",
    "Epoch 4/10\n",
    "938/938 [==============================] - ETA: 0s - loss: 8.3582\n",
    "Epoch 00004: val_loss improved from 8.56772 to 8.26433, saving model to /model/\n",
    "938/938 [==============================] - 34s 36ms/step - loss: 8.3582 - val_loss: 8.2643\n",
    "Start epoch 4.\n",
    "Learning rate: 0.009999999776482582\n",
    "Epoch 5/10\n",
    "935/938 [============================>.] - ETA: 0s - loss: 8.2523\n",
    "Epoch 00005: val_loss improved from 8.26433 to 8.25027, saving model to /model/\n",
    "938/938 [==============================] - 33s 35ms/step - loss: 8.2541 - val_loss: 8.2503\n",
    "Start epoch 5.\n",
    "Learning rate: 0.009999999776482582\n",
    "Epoch 6/10\n",
    "938/938 [==============================] - ETA: 0s - loss: 8.2502\n",
    "Epoch 00006: val_loss improved from 8.25027 to 8.25011, saving model to /model/\n",
    "938/938 [==============================] - 47s 50ms/step - loss: 8.2502 - val_loss: 8.2501\n",
    "Start epoch 6.\n",
    "Learning rate: 0.0009999999310821295\n",
    "Epoch 7/10\n",
    "936/938 [============================>.] - ETA: 0s - loss: 8.2506\n",
    "Epoch 00007: val_loss improved from 8.25011 to 8.25010, saving model to /model/\n",
    "938/938 [==============================] - 48s 51ms/step - loss: 8.2501 - val_loss: 8.2501\n",
    "Start epoch 7.\n",
    "Learning rate: 9.99999901978299e-05\n",
    "Epoch 8/10\n",
    "936/938 [============================>.] - ETA: 0s - loss: 8.2496\n",
    "Epoch 00008: val_loss improved from 8.25010 to 8.25010, saving model to /model/\n",
    "938/938 [==============================] - 59s 62ms/step - loss: 8.2501 - val_loss: 8.2501\n",
    "Start epoch 8.\n",
    "Learning rate: 9.99999883788405e-06\n",
    "Epoch 9/10\n",
    "938/938 [==============================] - ETA: 0s - loss: 8.2501\n",
    "Epoch 00009: val_loss did not improve from 8.25010\n",
    "938/938 [==============================] - 118s 126ms/step - loss: 8.2501 - val_loss: 8.2501\n",
    "Start epoch 9.\n",
    "Learning rate: 9.99999883788405e-07\n",
    "Epoch 10/10\n",
    "938/938 [==============================] - ETA: 0s - loss: 8.2501\n",
    "Epoch 00010: val_loss did not improve from 8.25010\n",
    "938/938 [==============================] - 119s 127ms/step - loss: 8.2501 - val_loss: 8.2501\n",
    "\n",
    "\n",
    "评估的结果：\n",
    "313/313 [==============================] - 10s 32ms/step - loss: 8.2501\n",
    "8.250103950500488"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
