{
 "cells": [
  {
   "cell_type": "raw",
   "id": "02fb7f49",
   "metadata": {},
   "source": [
    "小结：   \n",
    "    通过这节课的学习，我们了解了什么是注意力机制，并且了解了注意力的分类（空间、时间、通道、混合），并且手动实现了一个通道域的注意力机制，并且最后进行了实现。\n",
    "    \n",
    "    在之前的学习之中，我们学习了很多的网络模型，比如 CNN、RNN 等基本的网络模型，虽然这些模型是根据人的信息处理方式来进行设计并实现的，但是这些模型都有一些特点，那就是只是会根据输入的数据进行机械地输出。那么我们这节课便要来学习一下更加 “贴近人的信息处理方式的方法”———— 注意力机制。\n",
    "    \n",
    "    1. 什么是注意力机制\n",
    "    顾名思义，注意力机制，“Attention”，就是模仿人的注意力来进行网络模型的设计与实现。\n",
    "    我们每个人在日常生活之中，无时无刻不在使用着注意力，比如：\n",
    "        我们在看电视的时候会忽略掉电视周围的环境；\n",
    "        我们在学习的时候会对书本的注意力集中度较高；\n",
    "        我们在听音乐的时候对音乐本身的注意力较高，反而对周围的噪音注意力较小。\n",
    "    在神经网络之中采用注意力机制可以通过模仿人类的注意力行为，来对数据之中的重要的细节赋予更高的权重，反而对于一些不重要的细节来赋予较低的权重。\n",
    "    举个例子，如下图所示，有一只手拿着一朵花在以堆草丛之前，那么我们人在观察这种图片的时候，一般会将更多的注意力集中在这朵花和这只手上，而不是将注意力放在背景的草丛中。因此我们要让我们的网络模型学会如何使用注意力机制，从而其实将注意力更多地放在花和手上。\n",
    "    \n",
    "    2. 注意力的分类\n",
    "    注意力按照存在的地方大概可以分为四类：\n",
    "    空间注意力，就是我们上述图片所表述的注意力，它主要是强调我们在空间之上要注意哪些地方；\n",
    "    时间注意力，图片没有时间注意力，像音频、视频等连续的数据会使用到时间注意力，表示我们在哪个时间段要提高注意力；\n",
    "    通道注意力，众所周知，一般的图片包含三个通道：R、G、B，那么通道注意力就是强调在哪个通道之上给予更高的注意力权重；\n",
    "    混合注意力，使用上述两种及其以上的注意力，从而达到更好的效果。\n",
    "    在接下来的例子之中，我们会以通道注意力为例子进行演示如何使用注意力机制。\n",
    "   \n",
    "   3. 通道上的注意力机制的实现 ——SELayer\n",
    "    SENet 是一个使用通道注意力的模型，它可以对不同的通道求得不同的权重，进而对他们加权，从而实现通道域上的注意力机制。\n",
    "    SELayer 是 SENet 之中的一个网络层，是 SENet 的核心部分，我们可以将其单独摘出来作为一个通道域上的注意力。\n",
    "    在上图之中，我们可以发现，对于已经求得的特征（第二个正方体），SELayer 首先使用卷积网络，将其变为 1 * 1 * C 的特征，然后对于该特征进行一定的处理，处理结束之后的每一个通道的一个数字就代表着原特征图的相应通道的权重。最后我们将求得的权重乘到原特征上去便可以得到加权后的特征，这就表示我们已经在通道域上实现了注意力机制。\n",
    "    https://www.imooc.com/wiki/tensorflow/attentionmodel.html\n",
    "    在 TensorFlow 之中，我们可以通过继承 tf.keras.laysers.Layer 类来定义自己的网络层，于是我们可以将我们的 SELayer 定义为如下：\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "在初始化的函数之中，我们定义了我们需要用到的网络层以及相应的结构，\n",
    "通过 call 函数与初始化函数,我们可以得到该层的执行方式：\n",
    "    首先数据会经过一个全局平均池化，来变成一个 1* 1 * c 形状的特征；\n",
    "    然后经过我们定义的 FC 层，来计算出一个 1 * 1 * c 的权重，其中 FC 层包括；\n",
    "        一个全连接层；\n",
    "        一个 DropOut 层用于避免过拟合；\n",
    "        一个批次正则化层，这是便于更好地进行训练；\n",
    "        一个 relu 激活函数；\n",
    "        另外一个全连接层；\n",
    "        另外一个 DropOut 层；\n",
    "        另外一个批次正则化层；\n",
    "        一个 sigmoid 激活函数；\n",
    "    在得到权重之后，我们便使用矩阵的乘法，将原来的输出与权重相乘，从而得到在最终的结果。\n",
    "'''\n",
    "class SELayer(tf.keras.Model):\n",
    "    def __init__(self, filters, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.reduction = reduction\n",
    "        self.GAP = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.FC = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(units=self.filters // self.reduction, input_shape=(self.filters, )),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Dense(units=filters),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('sigmoid')\n",
    "        ])\n",
    "        self.Multiply = tf.keras.layers.Multiply()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.GAP(inputs)\n",
    "        x = self.FC(x)\n",
    "        x = self.Multiply([x, inputs])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61d20509",
   "metadata": {},
   "source": [
    "4. 使用通道注意力机制的完整代码\n",
    "    在定义了我们的注意力层之后，我们便可以着手将注意力机制应用到我们之前的任务之中，在这里我们以以前学习过的猫和狗分类为例子，添加我们的 Attention 机制，并且查看最终的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_download = os.path.dirname(tf.keras.utils.get_file('cats_and_dogs.zip', origin=dataset_url, extract=True))\n",
    "train_dataset_dir = path_download + '/cats_and_dogs_filtered/train'\n",
    "valid_dataset_dir = path_download + '/cats_and_dogs_filtered/validation'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_NUM = 2000\n",
    "VALID_NUM = 1000\n",
    "EPOCHS = 15\n",
    "Height = 128\n",
    "Width = 128\n",
    "\n",
    "train_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "valid_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data_generator = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                              directory=train_dataset_dir,\n",
    "                              shuffle=True,\n",
    "                              target_size=(Height, Width),\n",
    "                              class_mode='binary')\n",
    "valid_data_generator = valid_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                              directory=valid_dataset_dir,\n",
    "                              shuffle=True,\n",
    "                              target_size=(Height, Width),\n",
    "                              class_mode='binary')\n",
    "class SELayer(tf.keras.Model):\n",
    "    def __init__(self, filters, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.reduction = reduction\n",
    "        self.GAP = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.FC = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(units=self.filters // self.reduction, input_shape=(self.filters, )),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Dense(units=filters),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('sigmoid')\n",
    "        ])\n",
    "        self.Multiply = tf.keras.layers.Multiply()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.GAP(inputs)\n",
    "        x = self.FC(x)\n",
    "        x = self.Multiply([x, inputs])\n",
    "        return x\n",
    "\n",
    "    def build_graph(self, input_shape):\n",
    "        input_shape_without_batch = input_shape[1:]\n",
    "        self.build(input_shape)\n",
    "        inputs = tf.keras.Input(shape=input_shape_without_batch)\n",
    "        _ = self.call(inputs)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu',\n",
    "                input_shape=(Height, Width ,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    SELayer(16),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    SELayer(32),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    SELayer(64),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "       loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "       metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_data_generator,\n",
    "    steps_per_epoch=TRAIN_NUM // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_data_generator,\n",
    "    validation_steps=VALID_NUM // BATCH_SIZE)\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "loss=history.history['loss']\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs_ran = range(EPOCHS)\n",
    "\n",
    "plt.plot(epochs_ran, acc, label='Train Acc')\n",
    "plt.plot(epochs_ran, val_acc, label='Valid Acc')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs_ran, loss, label='Train Loss')\n",
    "plt.plot(epochs_ran, val_loss, label='Valid Loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58a3ff2e",
   "metadata": {},
   "source": [
    "Found 2000 images belonging to 2 classes.\n",
    "Found 1000 images belonging to 2 classes.\n",
    "\n",
    "Model: \"sequential_7\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_4 (Conv2D)            (None, 128, 128, 16)      448       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_4 (MaxPooling2 (None, 64, 64, 16)        0         \n",
    "_________________________________________________________________\n",
    "se_layer_3 (SELayer)         (None, 64, 64, 16)        117       \n",
    "_________________________________________________________________\n",
    "dropout_8 (Dropout)          (None, 64, 64, 16)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_5 (Conv2D)            (None, 64, 64, 32)        4640      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_5 (MaxPooling2 (None, 32, 32, 32)        0         \n",
    "_________________________________________________________________\n",
    "se_layer_4 (SELayer)         (None, 32, 32, 32)        298       \n",
    "_________________________________________________________________\n",
    "dropout_11 (Dropout)         (None, 32, 32, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_6 (Conv2D)            (None, 32, 32, 64)        18496     \n",
    "_________________________________________________________________\n",
    "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 64)        0         \n",
    "_________________________________________________________________\n",
    "se_layer_5 (SELayer)         (None, 16, 16, 64)        852       \n",
    "_________________________________________________________________\n",
    "dropout_14 (Dropout)         (None, 16, 16, 64)        0         \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 16384)             0         \n",
    "_________________________________________________________________\n",
    "dropout_15 (Dropout)         (None, 16384)             0         \n",
    "_________________________________________________________________\n",
    "dense_14 (Dense)             (None, 512)               8389120   \n",
    "_________________________________________________________________\n",
    "dropout_16 (Dropout)         (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_15 (Dense)             (None, 1)                 513       \n",
    "=================================================================\n",
    "Total params: 8,414,484\n",
    "Trainable params: 8,414,246\n",
    "Non-trainable params: 238\n",
    "_________________________________________________________________\n",
    "Epoch 1/15\n",
    "31/31 [==============================] - 56s 2s/step - loss: 0.7094 - accuracy: 0.5114 - val_loss: 0.6931 - val_accuracy: 0.5310\n",
    "Epoch 2/15\n",
    "31/31 [==============================] - 48s 2s/step - loss: 0.6930 - accuracy: 0.4990 - val_loss: 0.6927 - val_accuracy: 0.5869\n",
    "......\n",
    "Epoch 14/15\n",
    "31/31 [==============================] - 54s 2s/step - loss: 0.6174 - accuracy: 0.6348 - val_loss: 0.6309 - val_accuracy: 0.7240\n",
    "Epoch 15/15\n",
    "31/31 [==============================] - 47s 2s/step - loss: 0.6030 - accuracy: 0.6446 - val_loss: 0.6195 - val_accuracy: 0.7565\n",
    "\n",
    "    于是我们可以发现，我们的模型最终达到了 75% 的准确率，大家可以和之前的模型的结果做一个比较。\n",
    "    同时大家也可以根据自己对 CNN 和 MaxPooling 的理解来调整模型以及相应的参数，从而达到一个更好的效果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
