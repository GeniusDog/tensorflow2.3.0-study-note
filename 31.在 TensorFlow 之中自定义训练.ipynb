{
 "cells": [
  {
   "cell_type": "raw",
   "id": "844ab378",
   "metadata": {},
   "source": [
    "    在 TensorFlow 之中自定义训练\n",
    "    在前面两节的学习之中，我们学习了如何进行自定义微分操作，又学习了如何自定义模型，那么接下来这一小节我们便来学习如何进行自定义的最后一步 —— 自定义训练。\n",
    "    在之前的学习之中，当我们进行训练的时候，我们采用的都是 fit () 函数。虽然说 fit () 函数给我们提供了很多的选项，但是如果想要更加深入的定制我们的寻来你过程，那么我们便需要自己编写训练循环。\n",
    "    在这节课之中，我们会采用一个对 mnist 数据集进行分类的简单模型作为一个简单的示例来进行演示，以此来帮助大家理解自定义训练的过程。因此该课程主要可以分为以下几个部分：\n",
    "        - 自定义模型；\n",
    "        - 编写自定义循环；\n",
    "        - 如何在自定义循环中进行模型的优化工作。\n",
    "    1. 数据的准备工作\n",
    "    首先我们要准备好相应的 mnist 数据集，这里采用往常的处理方式：使用内置的 API 来获取数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# 数据归一化\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# 对数据进行了分批次的处理，批次的大小维 64 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "# 我们对训练数据进行了乱序处理\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "valid_dataset = valid_dataset.batch(64)\n",
    "\n",
    "'''\n",
    "2. 自定义模型\n",
    "    由于进行 Mnist 图像分类的任务比较简单，因此我们可以定义一个较为简单的模型，这里的模型的结构包含四层：\n",
    "    -Flattern 层：对二维数据进行展开；\n",
    "    -第一个 Dense 层：包含 128 个神经元；\n",
    "    -第二个 Dense 层：包含 64 个神经元；\n",
    "    -最后一个 Dense 分类层；包含 10 个神经元，对应于我们的十个分类。\n",
    "'''\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.l1 = tf.keras.layers.Flatten()\n",
    "        self.l2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.l3 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.l4 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.l1(inputs)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        y = self.l4(x)\n",
    "        return y\n",
    "model = MyModel()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27e8692b",
   "metadata": {},
   "source": [
    "    3. 定义训练循环\n",
    "    在做好准备工作之后，我们便来到了我们的最重要的部分，也就是如何进行自定义循环的构建。\n",
    "    在自定义循环之前，我们要先做好准备工作，分为如下几步：\n",
    "    自定义损失函数：在大多数情况之下，内置的损失函数以及足够我们使用，比如交叉熵等损失函数；\n",
    "    自定义优化器：优化器决定了按照如何的策略进行优化，我们最常用的优化器就是 Adam ，因此这里我们使用内置的 Adam 优化器；\n",
    "    （可选）定义变量监视器：用于监视我们的训练过程的各种参数，在这里我们只使用一个来监视我们的验证集合上的效果。\n",
    "    因此我们的代码可以如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ade3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# 监控验证机上的准确率\n",
    "val_acc = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a657a1a",
   "metadata": {},
   "source": [
    "然后我们便可以构建自定义循环，自定义循环大致分为以下几步：\n",
    "    编写一个循环 Epoch 次的循环，Epoch 为训练的循环数；\n",
    "    在循环内部对于数据集读取每一个 Batch，因为这里的 train_dataset 为可枚举的，因此我们直接使用枚举即可获得每一个批次的训练样本；\n",
    "    定义 tf.GradientTape () 梯度带；\n",
    "    在梯度带内进行模型的输出，以及损失的求取；\n",
    "    在梯度带外使用梯度带求得模型所有参数的梯度，在这里我们可以使用 model.trainable_weights 来获取所有可训练的参数；\n",
    "    使用优化器按照求得的梯度对模型的参数进行优化，这里直接使用 optimizer.apply_gradients 函数即可完成优化；\n",
    "    （可选）进行 Log 处理，打印出日志便于我们查看；\n",
    "    （可选）在每个 Epoch 的训练集的训练结束后，我们可以在测试集上查看结果，这里我们只查看了准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c14552",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start Training epoch \" + str(epoch))\n",
    "    \n",
    "    # 取出每一个批次的数据\n",
    "    for batch_i, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # 在梯度带内进行操作\n",
    "        with tf.GradientTape() as tape:\n",
    "          outputs = model(x_batch_train, training=True)\n",
    "          loss_value = loss_fn(y_batch_train, outputs)\n",
    "\n",
    "        # 求取梯度\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # 使用Optimizer进行优化\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log\n",
    "        if batch_i % 100 == 0:\n",
    "            print(\"Loss at batch %d: %.4f\" % (batch_i, float(loss_value)))\n",
    "\n",
    "    # 在验证集合上测试\n",
    "    for batch_i, (x_batch_train, y_batch_train) in enumerate(valid_dataset):\n",
    "        outputs = model(x_batch_train, training=False)\n",
    "        # 更新追踪器的状态\n",
    "        val_acc.update_state(y_batch_train, outputs)\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc.result()),))\n",
    "\n",
    "    # 重置追踪器\n",
    "    val_acc.reset_states()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b856f39",
   "metadata": {},
   "source": [
    "Start Training epoch 0\n",
    "Loss at batch 0: 0.1494\n",
    "Loss at batch 100: 0.2155\n",
    "Loss at batch 200: 0.1080\n",
    "Loss at batch 300: 0.0231\n",
    "Loss at batch 400: 0.1955\n",
    "Loss at batch 500: 0.2019\n",
    "Loss at batch 600: 0.0567\n",
    "Loss at batch 700: 0.1099\n",
    "Loss at batch 800: 0.0714\n",
    "Loss at batch 900: 0.0364\n",
    "Validation acc: 0.9691\n",
    "Start Training epoch 1\n",
    "Loss at batch 0: 0.0702\n",
    "Loss at batch 100: 0.0615\n",
    "Loss at batch 200: 0.0208\n",
    "Loss at batch 300: 0.0158\n",
    "Loss at batch 400: 0.0304\n",
    "Loss at batch 500: 0.1193\n",
    "Loss at batch 600: 0.0130\n",
    "Loss at batch 700: 0.1353\n",
    "Loss at batch 800: 0.1300\n",
    "Loss at batch 900: 0.0056\n",
    "Validation acc: 0.9715\n",
    "Start Training epoch 2\n",
    "Loss at batch 0: 0.0714\n",
    "Loss at batch 100: 0.0066\n",
    "Loss at batch 200: 0.0177\n",
    "Loss at batch 300: 0.0086\n",
    "Loss at batch 400: 0.0099\n",
    "Loss at batch 500: 0.1621\n",
    "Loss at batch 600: 0.1103\n",
    "Loss at batch 700: 0.0049\n",
    "Loss at batch 800: 0.0139\n",
    "Loss at batch 900: 0.0111\n",
    "Validation acc: 0.9754\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
