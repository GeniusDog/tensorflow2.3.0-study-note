{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d2a7f7cf",
   "metadata": {},
   "source": [
    "小结：\n",
    "    在本结中，认识了numpy，然后是numpy和tensorflow向量数据的转化、以及从“.npz”文件之中读取数据、读取文件之后搭建tensorflow的dataset数据集，这样的数据集tensorflow才能更好的处理，送入模型等。\n",
    "    在科学计算之中，Numpy 是一种必不可少的工具，因此我们在机器学习的过程之中难免会遇到使用 Numpy 数据的情况，所以我们有必要学习如何在 TensorFlow 之中学习如何使用 Numpy 数据。\n",
    "    1. 什么是 NumpyNumpy 是 Python 的一个扩展库。\n",
    "    支持高阶维度数组的复杂运算以及矩阵的运算。因此在进行科学计算的时候使用 Numpy 进行数据处理便会特别的方便。\n",
    "    在具体的使用过程之中，我们一般会遇到两种情况：\n",
    "        在内存中定义了 Numpy 数组，需要提供给 TesnorFlow 使用；\n",
    "        我们需要加载 Numpy 存放的文件的数据，也就是需要从“.npz”文件之中读取数据。\n",
    "    因此这节课之中我们就从两个方面来说明如何使用 Numpy 数据。\n",
    "    2. 在内存中使用 Numpy 数据\n",
    "    如果我们在内存中定义了 Numpy 数据，那么我们便可以通过 tf.convert_to_tensor() 函数来将 Numpy 数据转化为 Tensor，从而提供给 TensorFlow 使用。\n",
    "    比如以下示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ccc17ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(5, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_np = np.zeros((5, 3))\n",
    "x_tensor = tf.convert_to_tensor(x_np)\n",
    "\n",
    "print(type(x_np), type(x_tensor))\n",
    "print(x_tensor)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd36e602",
   "metadata": {},
   "source": [
    "    那如果我们需要将 Tensor 转化为 Numpy 呢？我们只需要使用 Tensor 对象中的 Numpy 函数即可将其转化为 Numpy 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02c87df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x_np_new=x_tensor.numpy()\n",
    "print(type(x_np_new))\n",
    "print(x_np_new)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a63ddf64",
   "metadata": {},
   "source": [
    "3. 从“.npz”文件之中读取数据\n",
    "    当我们要从 .npz 文件之中读取并使用数据的时候，我们大概要经过三个步骤：\n",
    "    打开 .npz 文件，并且其格式加载数据，要注意，不同的 .npz 文件中的格式都是由人为定义的，因此只有了解了文件中的组织格式，才能加载数据；\n",
    "    将加载的 .npz 数据转化为 tf.data.Dataset ；\n",
    "    进一步处理并使用该数据集合。\n",
    "    下面我们以一个简单的手写数字识别数据集（mnist）为例，来演示如何进行数据的加载与使用。\n",
    "    \n",
    " 3.1 打开文件并加载数据\n",
    "     这里用到的npz文件大家可以从谷歌仓库中下载，大家可以通过该链接下载。\n",
    "     然后我们需要首先得到下载文件的本地地址，在这里我假设地址是’/root/.keras/datasets/mnist.npz’。\n",
    "     该数据集是由一个字典组成，这个字典由四个元素组成，他们的key分别是：\n",
    "     x_train: 训练数据的图片数据；\n",
    "     y_train: 训练数据的标签；\n",
    "     x_test: 测试数据的图片数据；\n",
    "     y_test: 测试数据的标签。\n",
    "     了解了数据的结构后，我们便可以通过以下操作进行数据的加载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "path = '下载文件的地址'\n",
    "with np.load(path) as np_data:\n",
    "  train_exa = np_data['x_train']\n",
    "  train_labels = np_data['y_train']\n",
    "  test_exa = np_data['x_test']\n",
    "  test_labels = np_data['y_test']\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0816b850",
   "metadata": {},
   "source": [
    "3.2 构建tf.data.Dataset数据集\n",
    "    和之前一样，我们主要是通过 tf.data.Dataset.from_tensor_slices() 函数来构建数据集。\n",
    "    比如我们可以通过以下代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过 Numpy 数组正式构建了 tf.data.Dataset 数据集合\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_exa, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_exa, test_labels))\n",
    "\n",
    "print(train_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "538b7a9d",
   "metadata": {},
   "source": [
    "3.3 进一步的处理与使用\n",
    "当我们仅仅创建了数据集是远远不够的，我们还要进行进一步的处理，比如分批、打乱等基本操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8703bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里，我们对数据集合进行了乱序处理，然后将其按照 64 的大小进行批次的划分。\n",
    "train_dataset = train_dataset.shuffle(128).batch(64)\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "print(train_dataset, test_dataset)\n",
    "\n",
    "# 在接下来我们便可以使用该数据集，在这里我们可以使用一个简单的分类模型进行示例：\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, epochs=20)\n",
    "\n",
    "model.evaluate(test_dataset)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
