{
 "cells": [
  {
   "cell_type": "raw",
   "id": "06b0975a",
   "metadata": {},
   "source": [
    "    在之前的学习之中，我们已经简单地学习了如何进行文本数据的处理：\n",
    "    a.f.data.TextLineDataset 加载文本数据；\n",
    "    b.编码将数据进行编码。\n",
    "    而在这节课之中，我们将详细地了解将文本数据编码的各种方法，并且对最常用的字词嵌入方法进行深入研究，最后我们会给出一个完整的模型来对电影评价进行分类。\n",
    "\n",
    "    1. 文本数据编码的方法\n",
    "    在机器学习领域，所有模型的处理数据都是数字类型的数据。因此在文本处理的过程之中，如何将文本数据转化为数字类型数据是一个重要的研究课题。纵观所有的数据编码方法，我们可以将常用的编码方法大致可以分为三类：\n",
    "    a.简单编码处理；\n",
    "    b.简单编码后独热处理；\n",
    "    c.字词嵌入处理。\n",
    "    1.1 简单编码处理\n",
    "    假如我们有一个英文句子：s = \"How are you\"\n",
    "    那么我们便可以将字符串s编码为：s_1 = [0, 1, 2]\n",
    "    相应的“How you are”快就可以编码为：s_2 = [0, 2, 1]\n",
    "    这种编码方式最大的好处是简单，非常容易理解，而且占用的空间资源很少。\n",
    "    但是这种方法也有缺点：\n",
    "    那就是这种编码方式无法识别两个单词之间的关系，也就是说两个数字之间的关系不能代表实际的两个单词的关系，比如我们直观理解 How 和 You 的关系更近一些，然而 How 和 You 的编码距离是 2 ，而 are 和 you 的距离却是 1 。\n",
    "    同时也正是因为上面的原因，导致不同的编码规则会产生截然不同的效果，因为我们的“How are you”完全可以编码为 [111, 222, 333] ，而这会导致编码后的数据只能依靠固定的编码规则使用。\n",
    "    1.2 简单编码的独热化\n",
    "    这种编码和上述编码方式一样，只是进行了一些独热化处理，因为独热化处理能够让模型在分类任务上拥有更好的性质。\n",
    "    比如上述的例子：s = \"How are you\"\n",
    "    按照独热编码可以看作是：\n",
    "        \"How\" = [1, 0, 0]\n",
    "        \"are\" = [0, 1, 0]\n",
    "        \"you\" = [0, 0, 1]\n",
    "     虽然独热编码具有更好的性值，但是它仍然没有解决上面的问题，也就是说它依然无法表示两个单词之间的关系。不仅如此，它还引入了新的问题：存储空间开销巨大。因为每个单词的存储大小都是一个所有词汇量大小的一个数组。\n",
    "    1.3 字词嵌入\n",
    "    这种处理方式我们之前有过稍微的接触，字词嵌入会根据相关指定的参数来为每个单词生成一个固定长度的向量。\n",
    "    比如上面的英文句子,编码后可能变为：\n",
    "    s_3 = [[1.9, 0.4,-0.3],[0.74, 0.23, -0.3],[0.5, 0.6, 0.7]]\n",
    "    通过这种形式的编码处理，我们已经很难通过肉眼来看出原来的句子了，但是对于机器学习的网络模型来说，它却可以进行更快速的处理，同时它其中也包含着不同单词之间的距离信息。\n",
    "   \n",
    "   2. 使用 tf.keras.layers.Embedding 进行字词嵌入\n",
    "    该嵌入函数API的常用参数如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "这几个参数的具体含义包括：\n",
    "    input_dim: 输入的维度，对于字词嵌入来说就是词汇量的大小；\n",
    "    output_dim: 产出的维度，简单来说就是对单词嵌入产生的向量的长度；\n",
    "    embeddings_initializer： 如何对嵌入进行初始化；\n",
    "    embeddings_regularizer： 嵌入的正则化项，比如之前的L2正则化。\n",
    "'''\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim, output_dim, embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be8b4113",
   "metadata": {},
   "source": [
    "    通过这些参数，我们可以发现，我们在进行字词嵌入之前谓一需要做的就是找到词汇量的大小，而这一般是人为规定的。\n",
    "    我们可以通过一个简单的示例来看一下它是如何工作的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Embedding(100, 5)  # 100表示词汇量大小，5表示产出维度\n",
    "print(layer(tf.constant([1,2,3,4,5])).numpy())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40d8d674",
   "metadata": {},
   "source": [
    "我们可以得到输出,可以看到，我们的嵌入层已经成功进行了嵌入。\n",
    "    [[-0.00772538 -0.00696523 -0.0306471   0.01268767 -0.0099443 ]\n",
    "     [-0.00331452 -0.00279518 -0.03783524  0.00927589 -0.02038437]\n",
    "     [ 0.03577108  0.01887624 -0.00056656 -0.00773742  0.03503906]\n",
    "     [ 0.02601126  0.02511038  0.01170179 -0.02206317 -0.03981184]\n",
    "     [-0.00608523  0.03906326  0.02454172 -0.0453696  -0.00303098]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fedeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 使用内置API获取数据，同时规定最大的词汇量为10000\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "\n",
    "# 预处理：文本数据对齐\n",
    "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data, value=0, padding='post', maxlen=256)\n",
    "test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data, value=0, padding='post', maxlen=256)\n",
    "\n",
    "\n",
    "# 模型构建与编译\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 32),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练与测试\n",
    "history = model.fit(train_data, train_labels, epochs=30, batch_size=64)\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e10b6456",
   "metadata": {},
   "source": [
    "在云平台上的运行结果：\n",
    "同时我们也可以发现模型的参数主要集中在嵌入层当中。\n",
    "\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding (Embedding)        (None, None, 32)          320000    \n",
    "_________________________________________________________________\n",
    "global_average_pooling1d (Gl (None, 32)                0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 64)                2112      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1)                 65        \n",
    "=================================================================\n",
    "Total params: 322,177\n",
    "Trainable params: 322,177\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Epoch 1/30\n",
    "391/391 [==============================] - 34s 86ms/step - loss: 0.5116 - accuracy: 0.7684\n",
    "Epoch 2/30\n",
    "391/391 [==============================] - 34s 87ms/step - loss: 0.2571 - accuracy: 0.90051s - loss: 0.2\n",
    "Epoch 3/30\n",
    "391/391 [==============================] - 34s 86ms/step - loss: 0.2002 - accuracy: 0.9255\n",
    "Epoch 4/30\n",
    "391/391 [==============================] - 33s 85ms/step - loss: 0.1644 - accuracy: 0.9400\n",
    "Epoch 5/30\n",
    "391/391 [==============================] - 34s 88ms/step - loss: 0.1403 - accuracy: 0.9508\n",
    "Epoch 6/30\n",
    "391/391 [==============================] - 33s 84ms/step - loss: 0.1223 - accuracy: 0.9582\n",
    "Epoch 7/30\n",
    "391/391 [==============================] - 34s 86ms/step - loss: 0.1034 - accuracy: 0.9660\n",
    "Epoch 8/30\n",
    "391/391 [==============================] - 32s 83ms/step - loss: 0.0912 - accuracy: 0.9711\n",
    "Epoch 9/30\n",
    "391/391 [==============================] - 33s 86ms/step - loss: 0.0784 - accuracy: 0.9770\n",
    "Epoch 10/30\n",
    "391/391 [==============================] - 35s 89ms/step - loss: 0.0672 - accuracy: 0.9813\n",
    "Epoch 11/30\n",
    "391/391 [==============================] - 34s 87ms/step - loss: 0.0592 - accuracy: 0.9844\n",
    "Epoch 12/30\n",
    "391/391 [==============================] - 40s 102ms/step - loss: 0.0513 - accuracy: 0.9866\n",
    "Epoch 13/30\n",
    "391/391 [==============================] - 37s 95ms/step - loss: 0.0479 - accuracy: 0.9871\n",
    "Epoch 14/30\n",
    "391/391 [==============================] - 31s 78ms/step - loss: 0.0390 - accuracy: 0.9906\n",
    "Epoch 15/30\n",
    "391/391 [==============================] - 33s 83ms/step - loss: 0.0327 - accuracy: 0.9926\n",
    "Epoch 16/30\n",
    "391/391 [==============================] - 30s 78ms/step - loss: 0.0276 - accuracy: 0.9944\n",
    "Epoch 17/30\n",
    "391/391 [==============================] - 31s 80ms/step - loss: 0.0261 - accuracy: 0.9946\n",
    "Epoch 18/30\n",
    "391/391 [==============================] - 30s 78ms/step - loss: 0.0225 - accuracy: 0.9953\n",
    "Epoch 19/30\n",
    "391/391 [==============================] - 30s 77ms/step - loss: 0.0204 - accuracy: 0.9951\n",
    "Epoch 20/30\n",
    "391/391 [==============================] - 28s 72ms/step - loss: 0.0179 - accuracy: 0.9961\n",
    "Epoch 21/30\n",
    "391/391 [==============================] - 29s 74ms/step - loss: 0.0136 - accuracy: 0.9976\n",
    "Epoch 22/30\n",
    "391/391 [==============================] - 31s 79ms/step - loss: 0.0116 - accuracy: 0.9980\n",
    "Epoch 23/30\n",
    "391/391 [==============================] - 29s 75ms/step - loss: 0.0107 - accuracy: 0.9980\n",
    "Epoch 24/30\n",
    "391/391 [==============================] - 30s 78ms/step - loss: 0.0121 - accuracy: 0.9970\n",
    "Epoch 25/30\n",
    "391/391 [==============================] - 30s 78ms/step - loss: 0.0128 - accuracy: 0.9964\n",
    "Epoch 26/30\n",
    "391/391 [==============================] - 30s 76ms/step - loss: 0.0151 - accuracy: 0.9953\n",
    "Epoch 27/30\n",
    "391/391 [==============================] - 29s 75ms/step - loss: 0.0090 - accuracy: 0.9978\n",
    "Epoch 28/30\n",
    "391/391 [==============================] - 29s 75ms/step - loss: 0.0059 - accuracy: 0.9992\n",
    "Epoch 29/30\n",
    "391/391 [==============================] - 30s 76ms/step - loss: 0.0042 - accuracy: 0.9994\n",
    "Epoch 30/30\n",
    "391/391 [==============================] - 33s 85ms/step - loss: 0.0034 - accuracy: 0.9995\n",
    "782/782 [==============================] - 10s 13ms/step - loss: 1.6086 - accuracy: 0.8314\n",
    "\n",
    "[1.608591914176941, 0.8313999772071838]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
